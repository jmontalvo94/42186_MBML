{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"name":"04 - Probabilistic Programming with STAN.ipynb","provenance":[],"collapsed_sections":["tKGXkTS1qB-h","Rwcrky8WqB-x","JbvhPTVcqB-4","ZAK51za9qB_l"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"eaUht4qLqB8c","colab_type":"text"},"source":["# Week 4 - Probabilistic Programming with STAN"]},{"cell_type":"markdown","metadata":{"id":"n5ZCvcT0qB8e","colab_type":"text"},"source":["## Part 1 - Simple STAN model"]},{"cell_type":"markdown","metadata":{"id":"de5VCtOhqB8f","colab_type":"text"},"source":["Hello again. We've finally arrived to STAN - a platform for probabilistic programming!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VWFqNq4YqB8g","colab_type":"text"},"source":["...the usual imports...\n","\n","**well, notice the new packages!!**"]},{"cell_type":"code","metadata":{"id":"7J6KxqNfqEH8","colab_type":"code","outputId":"0074e8de-6193-446c-fa26-7cd90bfa7bdc","executionInfo":{"status":"ok","timestamp":1581948155038,"user_tz":-60,"elapsed":2482,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# First, we need to download an auxiliary Python file for STAN\n","!wget http://mlsm.man.dtu.dk/mbml/pystan_utils.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-02-17 14:02:33--  http://mlsm.man.dtu.dk/mbml/pystan_utils.py\n","Resolving mlsm.man.dtu.dk (mlsm.man.dtu.dk)... 192.38.87.226\n","Connecting to mlsm.man.dtu.dk (mlsm.man.dtu.dk)|192.38.87.226|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2661 (2.6K) [text/x-python]\n","Saving to: ‘pystan_utils.py’\n","\n","\rpystan_utils.py       0%[                    ]       0  --.-KB/s               \rpystan_utils.py     100%[===================>]   2.60K  --.-KB/s    in 0s      \n","\n","2020-02-17 14:02:34 (437 MB/s) - ‘pystan_utils.py’ saved [2661/2661]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1ZfK9y0dqB8h","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import pystan\n","import pystan_utils # our own small package with utilities to use with STAN\n","\n","# matplotlib options\n","plt.style.use('ggplot')\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (12, 8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_NmIZmhqB8l","colab_type":"code","colab":{}},"source":["# usually have to run this twice to take effect\n","plt.style.use('ggplot')\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (12, 8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OamWj6tFqB8p","colab_type":"text"},"source":["### 1.1. Hello World and variations"]},{"cell_type":"markdown","metadata":{"id":"tbsmpVQbqB8q","colab_type":"text"},"source":["Every respectable programming language requires that newcomers start with a small \"Hello World\" code. Besides being a familiar initial exercise, it also lets you learn a basic tool: how to print something! \n","\n","Don't forget. It may be useful for you later, for example for debugging!\n","\n","We'll do the program for you\n","\n"]},{"cell_type":"code","metadata":{"id":"kD7Z_K8rqB8r","colab_type":"code","outputId":"2bfd374f-889d-4fcf-a325-74bddbc5c79e","executionInfo":{"status":"ok","timestamp":1581948236422,"user_tz":-60,"elapsed":83639,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["Hello_STAN=\"\"\"\n","transformed data {\n","    print(\"Hello World from STAN\");\n","}\n","\"\"\"\n","\n","# compile model\n","sm = pystan.StanModel(model_code=Hello_STAN)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_525af6d7864d89b75150c90990c74893 NOW.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1iYwEu8cqB8x","colab_type":"text"},"source":["To run the program above in pystan, we need to call the following method. Notice the parameters."]},{"cell_type":"code","metadata":{"id":"2p2qaK9oqB8x","colab_type":"code","colab":{}},"source":["# run inference\n","fit = sm.sampling(algorithm=\"Fixed_param\", iter=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CQoM6VzGqB81","colab_type":"text"},"source":["Look carefully at your STAN output. It is NOT in this notebook... you'll have to go to the console to see it! \n","\n","Can you control the number of times the sentence \"Hello World...\" shows up? \n","\n","Can you try to print from other STAN code blocks? What are the effects?"]},{"cell_type":"code","metadata":{"id":"TQm-01JgqB83","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HLbP8HFjqB87","colab_type":"text"},"source":["The next step is to exchange data TO/FROM your STAN and Python codes, with a trivial model. \n","\n","The example below sends one real number to STAN, as data point a, then it gets it sends it back through two values: a single sample, x, from a normal distribution (centered at a); a new variable, b, that simply equals a."]},{"cell_type":"code","metadata":{"id":"uErzhV9dqB88","colab_type":"code","colab":{}},"source":["tofrom_STAN=\"\"\"\n","data {\n","    real a;\n","}\n","parameters {\n","    real x;\n","}\n","model {  \n","    x ~ normal(a, 10);\n","}\n","generated quantities {\n","    real b=a;\n","}\n","\"\"\"\n","\n","# compile model\n","sm = pystan.StanModel(model_code=tofrom_STAN)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZBirmikqB9A","colab_type":"text"},"source":["This time, we will use HMC (Hamiltonian Monte Carlo)"]},{"cell_type":"code","metadata":{"id":"VdvL4Au6qB9B","colab_type":"code","colab":{}},"source":["# run inference\n","fit = sm.sampling(data={'a':5}, algorithm=\"HMC\", seed=0, iter=1000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1vjEcUzqB9F","colab_type":"text"},"source":["Take a look at the results (variable fit). "]},{"cell_type":"code","metadata":{"id":"GXW-MVZWqB9G","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUAauI1KqB9J","colab_type":"text"},"source":["Actually, there's still more to it. In fact, STAN didn't just draw a single value x as you can see from the table (look at n_eff, it obtained 2000 draws!). All the 2000 samples are available through fit.extract()"]},{"cell_type":"code","metadata":{"id":"uYEmcmxoqB9K","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5kWu_885qB9N","colab_type":"text"},"source":["Make a histogram for x. What kind of distribution would you expect? ;-)"]},{"cell_type":"code","metadata":{"id":"LSTROVSSqB9O","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"byKc6HPWqB9Q","colab_type":"text"},"source":["What's happening is that each single iteration (of 1000) of each single chain (of 4) is drawing one sample. Since STAN only uses data after the first 50% iterations, it becomes 500x4=2000 samples of the same variable. That is why x is not a single value, but instead a distribution. "]},{"cell_type":"markdown","metadata":{"id":"37rxMP8SqB9R","colab_type":"text"},"source":["Actually you could have used \n",">fit.traceplot('x')"]},{"cell_type":"code","metadata":{"id":"_0-jUaoPqB9U","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0tibQcs6qB9b","colab_type":"text"},"source":["Let's ellaborate a bit more with the input/outputs."]},{"cell_type":"code","metadata":{"id":"8uE0mxk5qB9d","colab_type":"code","colab":{}},"source":["simple_STAN=\"\"\"\n","data {\n","    real a;\n","}\n","transformed data {\n","    real b=2*a;\n","    int c=10;\n","    print(\"c=\", c, \" b=\", b); // check this in the console!!!\n","}\n","parameters {\n","    real x;\n","}\n","model {\n","    x ~ normal(a, 10);\n","}\n","generated quantities {\n","    real d=a+c;\n","    real e=4.0;\n","}\n","\"\"\"\n","\n","# compile model\n","sm = pystan.StanModel(model_code=simple_STAN)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8VaTBcy8qB9h","colab_type":"text"},"source":["Let's run it"]},{"cell_type":"code","metadata":{"id":"CJfiurqoqB9i","colab_type":"code","colab":{}},"source":["# run inference\n","fit = sm.sampling(data={'a':5}, algorithm=\"HMC\", seed=0, iter=1000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FaSDpz-1qB9l","colab_type":"text"},"source":["Notice that the function call returns a value into the 'fit' variable. Do you want to take a look at it? Can you retrieve the value of the parameter 'b'? What about 'd'?"]},{"cell_type":"code","metadata":{"id":"ltkwD-JTqB9m","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lnV21soqB9s","colab_type":"text"},"source":["With one data point at a time, we won't go that far, so it's time to work with vectors...  "]},{"cell_type":"code","metadata":{"id":"lceoU5nuqB9s","colab_type":"code","colab":{}},"source":["simple_vector_STAN=\"\"\"\n","data {\n","    int N;\n","    vector[N] a;\n","}\n","transformed data {\n","    real b=mean(a);\n","    print(\"b=\", b); // check this in the console!!!\n","}\n","parameters {\n","    real x;\n","    real y;\n","}\n","model {\n","    x ~ normal(b, 10);\n","    a ~ normal(y, 10);\n","}\n","\"\"\"\n","\n","# compile model\n","sm = pystan.StanModel(model_code=simple_vector_STAN)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jO7nJIt9qB9x","colab_type":"code","colab":{}},"source":["a = np.array([1.0, 3.1, 3.4, 6.7, 4.0, 0.0, 1.1, 2.0])\n","\n","# run inference\n","fit = sm.sampling(data={'N':len(a), 'a':a}, algorithm=\"HMC\", seed=0, iter=1000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2QeEvlA1qB9z","colab_type":"text"},"source":["Try to do a traceplot. Can you interpret the results?"]},{"cell_type":"code","metadata":{"id":"ydH-2QtjqB91","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EILm3vZ2qB94","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Da3qBfPqB97","colab_type":"text"},"source":["## 2 Your first STAN model\n","\n","\n","### 2.1 The cyclist problem\n","\n","Let's do the cyclist problem from the lecture slides: We have a dataset of observations (travel times of bicycle trips), and we want to estimate its distribution, assuming it follows a Gaussian curve of some form. \n","\n","\n","\n","As detailed in the lecture slides, you can code your STAN model directly into a Python string\n"]},{"cell_type":"code","metadata":{"id":"1K_Z9deXqB98","colab_type":"code","colab":{}},"source":["cyclist_STAN=\"\"\"\n","data {\n","    int<lower=0> N; // number of samples\n","    vector[N] tt;   // observed travel times\n","}\n","parameters {\n","    real at;          // average travel time\n","    real<lower=0> tu; // traffic uncertainty\n","}\n","model {\n","    at ~ normal(12, 10);\n","    tu ~ cauchy(0, 10);\n","    tt ~ normal(at, tu);\n","}\n","\"\"\"\n","\n","# compile model\n","sm = pystan.StanModel(model_code=cyclist_STAN)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUicSK6ZqB9_","colab_type":"text"},"source":["It is always good to have an intuitive notion of the prior forms (e.g. are they \"too\" informative, maybe? Or too wide?)\n","\n","Can you plot the priors for _at_ and _tu_?"]},{"cell_type":"code","metadata":{"id":"MrbIc1qvqB-A","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wI8sk0ttqB-D","colab_type":"text"},"source":["Given what we know of the problem, do they make sense? \n"]},{"cell_type":"markdown","metadata":{"id":"Q5-6con4qB-E","colab_type":"text"},"source":["\n","\n","Below is the Python code that calls for the STAN code above. "]},{"cell_type":"code","metadata":{"id":"pZhOvHuEqB-F","colab_type":"code","colab":{}},"source":["# store data in python dictionary\n","cyclist_dat = {'N': 14,\n","               'tt': [13,17,16,32,12,13,28,12,14,18,36,16,16,31]}\n","\n","# run inference\n","fit = sm.sampling(data=cyclist_dat, algorithm=\"NUTS\", seed=0, iter=1000)\n","\n","# show results\n","print(fit)\n","\n","# extract samples\n","samples = fit.extract(permuted=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4AsCIy7WqB-J","colab_type":"text"},"source":["The resulting MCMC samples that you get are in the \"samples\" object that we created above to collect all fit.extract() data. Try to **inspect** it, particularly do a histogram for the \"at\" and \"tu\" variables.\n","\n","Check how many samples you have, and relate the number with the statistics that STAN provides above:\n","- How many samples did it actually generate?\n","- How many is it using for the final statistics?\n","- How many chains is it using (and how many samples being used in the end by each chain?)\n","\n"]},{"cell_type":"code","metadata":{"id":"Aabgg3JKqB-J","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N3XcjrxcqB-M","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8XtjE-s_qB-P","colab_type":"text"},"source":["To add to STAN's plot that you've used already, and we put some standard outout code here for you. ;-)"]},{"cell_type":"code","metadata":{"id":"2ur45Td5qB-R","colab_type":"code","colab":{}},"source":["from scipy import stats\n","\n","print(\"\\nAverate travel time:\")\n","print(\"posterior mode=%.2f mean=%.2f std=%.2f\" % (stats.mode(samples[\"at\"].round(0))[0][0], samples['at'].mean(), samples['at'].std()))\n","\n","print(\"\\nTraffic uncertainty:\")\n","print(\"posterior mode=%.2f mean=%.2f std=%.2f\" % (stats.mode(samples[\"tu\"].round(1))[0][0], samples['tu'].mean(), samples['tu'].std()))\n","\n","print (\"\\nLog marginal likelihood:\", samples[\"lp__\"].mean())\n","\n","fit.plot()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddYi_-tIqB-g","colab_type":"text"},"source":["**note:** the concept of \"Log marginal likelihood\" corresponds to the marginal probability of all data (denominator in Bayes formula)"]},{"cell_type":"markdown","metadata":{"id":"4JjrjZE0qB-h","colab_type":"text"},"source":["\n","\n","Redefine the priors above and re-estimate the model (try only a couple of extreme values, just to see the effect)"]},{"cell_type":"markdown","metadata":{"id":"tKGXkTS1qB-h","colab_type":"text"},"source":["### 2.2 Obtaining useful results from your model"]},{"cell_type":"markdown","metadata":{"id":"OiV7fWqEqB-i","colab_type":"text"},"source":["Using the generated MCMC samples, you can find some useful answers.  For example, can you calculate the probability that the next trip takes less than 18 minutes?"]},{"cell_type":"code","metadata":{"id":"w4XLd5zDqB-j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wEhbtixqB-l","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jUuGDvsqB-o","colab_type":"text"},"source":["Notice that the certainty of your answer depends on the number of samples... \n","\n","Alternatively, we can use the \"generated quantities\" block of the STAN program to compute this probability. Lets extend our previous STAN program with a \"generated quantities\" block to compute the probability that the next trip takes less than 18 minutes."]},{"cell_type":"code","metadata":{"id":"es3RuecuqB-p","colab_type":"code","colab":{}},"source":["cyclist_STAN=\"\"\"\n","data {\n","    int<lower=0> N; // number of samples\n","    vector[N] tt;   // observed travel times\n","}\n","parameters {\n","    real at;          // average travel time\n","    real<lower=0> tu; // traffic uncertainty\n","}\n","model {\n","    at ~ normal(12, 10);\n","    tu ~ cauchy(0, 10);\n","    tt ~ normal(at, tu);\n","}\n","generated quantities {\n","    real prediction;\n","    int<lower=0,upper=1> is_less;    \n","    \n","    prediction = normal_rng(at, tu); // make prediction\n","    if (prediction < 18)             // check if prediction is less then 18\n","        is_less = 1;\n","    else\n","        is_less = 0;\n","}\n","\"\"\"\n","\n","# compile model\n","sm = pystan.StanModel(model_code=cyclist_STAN)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wMxu62mOqB-q","colab_type":"text"},"source":["Analyse the code above carefully. \n","\n","Notice how the \"generated quantities\" follows closely the generative process to sample observed travel times (predictions).\n","\n","Run inference given some observed data:"]},{"cell_type":"code","metadata":{"id":"qiB20B_xqB-r","colab_type":"code","colab":{}},"source":["# store data in python dictionary\n","cyclist_dat = {'N': 14,\n","               'tt': [13,17,16,32,12,13,28,12,14,18,36,16,16,31]}\n","\n","# run inference\n","fit = sm.sampling(data=cyclist_dat, algorithm=\"NUTS\", seed=0, iter=1000)\n","\n","# show results\n","print(fit)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eo7CnF05qB-w","colab_type":"text"},"source":["Did you get a similar result?"]},{"cell_type":"markdown","metadata":{"id":"Rwcrky8WqB-x","colab_type":"text"},"source":["### 2.3 Revised cyclist model: mixture model\n","\n","We will now consider a more realistic model. Since sometimes cyclists are prone to extraordinary circumstances (e.g. flat tire, forgot something at home, ran into a friend and started chatting, etc.), the distribution of travel times can be bimodal, such that on certain days (\"abnormal\" days) the distribution of travel times is radically different the distribution of \"normal\" days. \n","\n","We will formulate this assumption as mixture of two Gaussians. \n","\n","Can you write the new (revised) model in STAN?\n","\n","Hints:\n","- We will now have two Gaussians instead of one;\n","- We need a mixture parameter $\\pi$ (real parameter between 0 and 1) that controls the mixing proportions of the two Gaussians (see lecture slides);\n","- The likelihood is now of the form: \n","\n","$\\pi \\, \\mathcal{N}(at_o,tu_o) + (1-\\pi) \\, \\mathcal{N}(at_a,tu_a)$\n","\n","We can encode the likelihood in STAN using the function \"log_mix()\" as follows:\n","\n","```python\n","for (n in 1:N)\n","    target += log_mix(pi, normal_log(tt[n], at_o, tu_o), normal_log(tt[n], at_a, tu_a));\n","```\n","\n","The ```target``` variable is simply the log joint probability of the model. This is essentially what STAN needs to know about the model in order to perform inference. Therefore, if you write ```at ~ normal(10, 10)``` in the model block of your STAN program, that is actually equivalent to writting:\n","\n","```python\n","for (n in 1:N)\n","    target += normal_log(at, 10, 10);\n","```\n","\n","The former style is much more intuitive. However, in some cases (like this one), we need to directly increment the ```target``` quantity in order to exploit more advanced functionality of STAN. "]},{"cell_type":"code","metadata":{"id":"rbC6kBjUqB-y","colab_type":"code","colab":{}},"source":["cyclist_STAN_mixture = \"\"\"\n","data {\n","    int<lower=0> N; // number of samples\n","    vector[N] tt;   // observed travel times\n","}\n","parameters {\n","    // YOUR CODE HERE\n","}\n","model {\n","    // YOUR CODE HERE\n","}\n","\"\"\"\n","\n","# compile model\n","sm = pystan.StanModel(model_code=cyclist_STAN_mixture)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_nK2nqqsqB-0","colab_type":"text"},"source":["Lets run inference on the revised model using the same cyclist data from before:"]},{"cell_type":"code","metadata":{"id":"pf3EXw4ZqB-1","colab_type":"code","colab":{}},"source":["# store data in python dictionary\n","cyclist_dat = {'N': 14,\n","               'tt': [13,17,16,32,12,13,28,12,14,18,36,16,16,31]}\n","\n","# run inference\n","fit = sm.sampling(data=cyclist_dat, algorithm=\"NUTS\", seed=0, chains=1, iter=1000)\n","\n","# show results\n","print(fit)\n","\n","# extract samples\n","samples = fit.extract(permuted=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sVuLm3BfqB-3","colab_type":"text"},"source":["Look at the results. What can you tell?"]},{"cell_type":"markdown","metadata":{"id":"JbvhPTVcqB-4","colab_type":"text"},"source":["### 2.4 Obtaining useful results from your revised model"]},{"cell_type":"markdown","metadata":{"id":"Akc3R9ILqB-6","colab_type":"text"},"source":["As you did for the previous model with a single Gaussian, can you compute the probability that the next trip takes less than 18 minutes (given the new and more realistic model)?"]},{"cell_type":"code","metadata":{"id":"jIP2rq-hqB--","colab_type":"code","colab":{}},"source":["cyclist_STAN_mixture = \"\"\"\n","data {\n","    int<lower=0> N; // number of samples\n","    vector[N] tt;   // observed travel times\n","}\n","parameters {\n","    // SAME CODE AS YOU WROTE BEFORE IN 2.3\n","}\n","model {\n","    // SAME CODE AS YOU WROTE BEFORE IN 2.3\n","}\n","generated quantities {\n","    // WRITE YOUR NEW CODE HERE\n","}\n","\"\"\"\n","\n","# compile model\n","sm = pystan.StanModel(model_code=cyclist_STAN_mixture)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZNMZVjiKqB_B","colab_type":"text"},"source":["Lets run inference on the model and check our answer:"]},{"cell_type":"code","metadata":{"id":"p-BevosXqB_D","colab_type":"code","colab":{}},"source":["# store data in python dictionary\n","cyclist_dat = {'N': 14,\n","               'tt': [13,17,16,32,12,13,28,12,14,18,36,16,16,31]}\n","\n","# run inference\n","fit = sm.sampling(data=cyclist_dat, algorithm=\"NUTS\", seed=0, iter=1000)\n","\n","# show results\n","print(fit)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0m38rWC3qB_J","colab_type":"text"},"source":["Did you notice how different the answer when compared to previous model (single Gaussian)? Which one makes more sense to you? "]},{"cell_type":"markdown","metadata":{"id":"FNR1sItjqB_J","colab_type":"text"},"source":["## Part 2 -  Mixture model"]},{"cell_type":"markdown","metadata":{"id":"soKrcDpMqB_M","colab_type":"text"},"source":["We know that by know you think you've had enough about Mixture Models... but in fact, there's (even) much more to it! :-)\n","\n","For example, your well-known K-Means clustering algorithm is in fact a special case of a mixture model. \n","\n","Let's create a dataset with 200 samples that are distributed around 2 centers with 0.6 standard deviation. We will use the function [`make_blobs` ](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)that generates isotropic Gaussian blobs for clustering."]},{"cell_type":"code","metadata":{"id":"YaLmNbGVqB_P","colab_type":"code","colab":{}},"source":["# Generate some data\n","from sklearn.datasets.samples_generator import make_blobs\n","X, y_true = make_blobs(n_samples=100, centers=2, cluster_std=0.60, random_state=0)\n","X = X[:, ::-1] # flip axes for better plotting"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"br7jBhl7qB_Q","colab_type":"code","colab":{}},"source":["# Plot the data with K Means Labels\n","from sklearn.cluster import KMeans\n","kmeans = KMeans(2, random_state=0)\n","labels = kmeans.fit(X).predict(X)\n","sns.scatterplot(X[:, 0], X[:, 1], hue=labels)\n","plt.title('Dataset with k-means labels');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tuGWMyY7qB_S","colab_type":"text"},"source":["Let's look at their centers"]},{"cell_type":"code","metadata":{"id":"CWQMac0hqB_T","colab_type":"code","colab":{}},"source":["kmeans.cluster_centers_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCX9lEJOqB_a","colab_type":"text"},"source":["It's time for our STAN GMM model. Get ready..."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"lxUwH2LvqB_a","colab_type":"text"},"source":["Study the code carefully"]},{"cell_type":"code","metadata":{"id":"ijOnWTY9qB_b","colab_type":"code","colab":{}},"source":["GMM_STAN=\"\"\"\n","data {\n","    int<lower=0> D;  // number of dimensions\n","    int<lower=0> N;  // number of samples\n","    vector[D] X[N] ; // observed points\n","    int<lower=0> K;  //number of clusters\n","}\n","\n","parameters {\n","    simplex[K] theta;             // mixing proportions\n","    vector[D] mu[K];              // mixture component means\n","    vector<lower=0>[D] sigma[K];  // covariance matrices\n","}\n","model {\n","    vector[K] log_theta = log(theta);  // cache log calculation\n","    \n","    for (k in 1:K){\n","        mu[k] ~ normal(0, 10);\n","        sigma[k] ~ cauchy(0, 10);\n","    }\n","    \n","    for (n in 1:N) {\n","        vector[K] lps = log_theta;\n","        for (k in 1:K)\n","            lps[k] += normal_lpdf(X[n] | mu[k], sigma[k]);\n","        target += log_sum_exp(lps);\n","    }\n","}\n","\"\"\"\n","\n","# compile model\n","sm = pystan.StanModel(model_code=GMM_STAN)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7YngbS0qB_d","colab_type":"text"},"source":["Inspect carefully the following code too. Particularly, notice that we're running with a single chain and 4000 iterations, instead of the default (chains=4, iterations=1000). Why would that be? \n","\n","To answer this question, run this code and check the results (compare with the centers above)."]},{"cell_type":"code","metadata":{"id":"pXzrC3ckqB_d","colab_type":"code","colab":{}},"source":["K=2\n","N=len(X)\n","D=2\n","\n","data={'X':X, 'N': N, 'K':K, 'D':2}\n","\n","# run inference\n","fit = sm.sampling(data=data, algorithm=\"NUTS\", chains=1, seed=0, iter=4000)\n","\n","# show results\n","print(fit)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fpFvf1_QqB_f","colab_type":"text"},"source":["Now, try to put the default values (chains=4, iterations=1000). What's the problem? \n"]},{"cell_type":"code","metadata":{"id":"_sqcMzkRqB_f","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dBto9vZFqB_g","colab_type":"text"},"source":["Create and test a new version of the code above with higher K (e.g. K=3). "]},{"cell_type":"code","metadata":{"id":"OGKOO7DoqB_h","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAK51za9qB_l","colab_type":"text"},"source":["### Bonus: Alternative way of doing inference using ADVI"]},{"cell_type":"code","metadata":{"id":"s6wJLMv7qB_l","colab_type":"code","outputId":"6a863836-7850-4d87-a169-0f5f411f4f51","colab":{}},"source":["fit = sm.vb(data=data, iter=10000, algorithm=\"fullrank\", grad_samples=10, seed=42, verbose=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:pystan:Automatic Differentiation Variational Inference (ADVI) is an EXPERIMENTAL ALGORITHM.\n","WARNING:pystan:ADVI samples may be found on the filesystem in the file `/var/folders/sz/2168fj0j081bv6vc6cgg3mc40000gn/T/tmpovih1tww/output.csv`\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Ff9djdh9qB_n","colab_type":"text"},"source":["Did you notice how much faster that was? Did it get the right result? Let's see:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"1nJlT56CqB_o","colab_type":"code","outputId":"6aca3b41-9ec1-4edb-a2d7-7b08f7500a9b","colab":{}},"source":["mus = pystan_utils.vb_extract_variable(fit, \"mu\", var_type=\"matrix\", dims=[2,2])\n","print(\"mus:\", mus)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mus: [[1.01784005 2.05098601]\n"," [4.330718   0.95910048]]\n"],"name":"stdout"}]}]}